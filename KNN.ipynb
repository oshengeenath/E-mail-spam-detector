{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0  WORD_FREQ_MAKE  WORD_FREQ_ADDRESS  WORD_FREQ_ALL  \\\n4205        4205        0.685518          -0.248027       0.637104   \n4206        4206       -0.347922          -0.248027      -0.565245   \n4207        4207        0.652181          -0.248027       0.016537   \n4208        4208        2.852406          -0.248027      -0.565245   \n4209        4209       -0.347922          -0.248027       0.695282   \n\n      WORD_FREQ_3D  WORD_FREQ_OUR  WORD_FREQ_OVER  WORD_FREQ_REMOVE  \\\n4205     -0.046644      -0.473040        0.772994         -0.295731   \n4206     -0.046644      -0.473040       -0.350205         -0.295731   \n4207     -0.046644      -0.473040       -0.350205         -0.295731   \n4208     -0.046644      -0.007737       -0.350205         -0.295731   \n4209     -0.046644      -0.473040       -0.350205         -0.295731   \n\n      WORD_FREQ_INTERNET  WORD_FREQ_ORDER  ...  CHAR_FREQ_;  CHAR_FREQ_(  \\\n4205           -0.263265        -0.325617  ...    -0.160010     0.320730   \n4206           -0.263265        -0.325617  ...    -0.160010    -0.525294   \n4207           -0.263265        -0.325617  ...     0.243945     2.093004   \n4208           -0.263265        -0.325617  ...    -0.160010    -0.317435   \n4209           -0.263265        -0.325617  ...    -0.160010    -0.525294   \n\n      CHAR_FREQ_[  CHAR_FREQ_!  CHAR_FREQ_$  CHAR_FREQ_#  \\\n4205    -0.164364    -0.333407    -0.317325    -0.105071   \n4206    -0.164364     0.085226    -0.317325    -0.105071   \n4207    -0.164364    -0.333407    -0.317325    -0.105071   \n4208    -0.164364    -0.333407    -0.317325    -0.105071   \n4209    -0.164364    -0.185166    -0.317325    -0.105071   \n\n      CAPITAL_RUN_LENGTH_AVERAGE  CAPITAL_RUN_LENGTH_LONGEST  \\\n4205                   -0.127986                   -0.246243   \n4206                   -0.115525                   -0.241232   \n4207                   -0.120081                   -0.231210   \n4208                   -0.127835                   -0.236221   \n4209                   -0.124727                   -0.236221   \n\n      CAPITAL_RUN_LENGTH_TOTAL  SPAM  \n4205                 -0.328464     0  \n4206                 -0.448093     0  \n4207                 -0.279966     0  \n4208                 -0.344630     0  \n4209                 -0.406061     0  \n\n[5 rows x 59 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>WORD_FREQ_MAKE</th>\n      <th>WORD_FREQ_ADDRESS</th>\n      <th>WORD_FREQ_ALL</th>\n      <th>WORD_FREQ_3D</th>\n      <th>WORD_FREQ_OUR</th>\n      <th>WORD_FREQ_OVER</th>\n      <th>WORD_FREQ_REMOVE</th>\n      <th>WORD_FREQ_INTERNET</th>\n      <th>WORD_FREQ_ORDER</th>\n      <th>...</th>\n      <th>CHAR_FREQ_;</th>\n      <th>CHAR_FREQ_(</th>\n      <th>CHAR_FREQ_[</th>\n      <th>CHAR_FREQ_!</th>\n      <th>CHAR_FREQ_$</th>\n      <th>CHAR_FREQ_#</th>\n      <th>CAPITAL_RUN_LENGTH_AVERAGE</th>\n      <th>CAPITAL_RUN_LENGTH_LONGEST</th>\n      <th>CAPITAL_RUN_LENGTH_TOTAL</th>\n      <th>SPAM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4205</th>\n      <td>4205</td>\n      <td>0.685518</td>\n      <td>-0.248027</td>\n      <td>0.637104</td>\n      <td>-0.046644</td>\n      <td>-0.473040</td>\n      <td>0.772994</td>\n      <td>-0.295731</td>\n      <td>-0.263265</td>\n      <td>-0.325617</td>\n      <td>...</td>\n      <td>-0.160010</td>\n      <td>0.320730</td>\n      <td>-0.164364</td>\n      <td>-0.333407</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.127986</td>\n      <td>-0.246243</td>\n      <td>-0.328464</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4206</th>\n      <td>4206</td>\n      <td>-0.347922</td>\n      <td>-0.248027</td>\n      <td>-0.565245</td>\n      <td>-0.046644</td>\n      <td>-0.473040</td>\n      <td>-0.350205</td>\n      <td>-0.295731</td>\n      <td>-0.263265</td>\n      <td>-0.325617</td>\n      <td>...</td>\n      <td>-0.160010</td>\n      <td>-0.525294</td>\n      <td>-0.164364</td>\n      <td>0.085226</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.115525</td>\n      <td>-0.241232</td>\n      <td>-0.448093</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4207</th>\n      <td>4207</td>\n      <td>0.652181</td>\n      <td>-0.248027</td>\n      <td>0.016537</td>\n      <td>-0.046644</td>\n      <td>-0.473040</td>\n      <td>-0.350205</td>\n      <td>-0.295731</td>\n      <td>-0.263265</td>\n      <td>-0.325617</td>\n      <td>...</td>\n      <td>0.243945</td>\n      <td>2.093004</td>\n      <td>-0.164364</td>\n      <td>-0.333407</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.120081</td>\n      <td>-0.231210</td>\n      <td>-0.279966</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4208</th>\n      <td>4208</td>\n      <td>2.852406</td>\n      <td>-0.248027</td>\n      <td>-0.565245</td>\n      <td>-0.046644</td>\n      <td>-0.007737</td>\n      <td>-0.350205</td>\n      <td>-0.295731</td>\n      <td>-0.263265</td>\n      <td>-0.325617</td>\n      <td>...</td>\n      <td>-0.160010</td>\n      <td>-0.317435</td>\n      <td>-0.164364</td>\n      <td>-0.333407</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.127835</td>\n      <td>-0.236221</td>\n      <td>-0.344630</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4209</th>\n      <td>4209</td>\n      <td>-0.347922</td>\n      <td>-0.248027</td>\n      <td>0.695282</td>\n      <td>-0.046644</td>\n      <td>-0.473040</td>\n      <td>-0.350205</td>\n      <td>-0.295731</td>\n      <td>-0.263265</td>\n      <td>-0.325617</td>\n      <td>...</td>\n      <td>-0.160010</td>\n      <td>-0.525294</td>\n      <td>-0.164364</td>\n      <td>-0.185166</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.124727</td>\n      <td>-0.236221</td>\n      <td>-0.406061</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 59 columns</p>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"pcadata.csv\")\n",
    "data.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "(4210, 59)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "0    2531\n1    1679\nName: SPAM, dtype: int64"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"SPAM\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4210 entries, 0 to 4209\n",
      "Data columns (total 59 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Unnamed: 0                  4210 non-null   int64  \n",
      " 1   WORD_FREQ_MAKE              4210 non-null   float64\n",
      " 2   WORD_FREQ_ADDRESS           4210 non-null   float64\n",
      " 3   WORD_FREQ_ALL               4210 non-null   float64\n",
      " 4   WORD_FREQ_3D                4210 non-null   float64\n",
      " 5   WORD_FREQ_OUR               4210 non-null   float64\n",
      " 6   WORD_FREQ_OVER              4210 non-null   float64\n",
      " 7   WORD_FREQ_REMOVE            4210 non-null   float64\n",
      " 8   WORD_FREQ_INTERNET          4210 non-null   float64\n",
      " 9   WORD_FREQ_ORDER             4210 non-null   float64\n",
      " 10  WORD_FREQ_MAIL              4210 non-null   float64\n",
      " 11  WORD_FREQ_RECEIVE           4210 non-null   float64\n",
      " 12  WORD_FREQ_WILL              4210 non-null   float64\n",
      " 13  WORD_FREQ_PEOPLE            4210 non-null   float64\n",
      " 14  WORD_FREQ_REPORT            4210 non-null   float64\n",
      " 15  WORD_FREQ_ADDRESSES         4210 non-null   float64\n",
      " 16  WORD_FREQ_FREE              4210 non-null   float64\n",
      " 17  WORD_FREQ_BUSINESS          4210 non-null   float64\n",
      " 18  WORD_FREQ_EMAIL             4210 non-null   float64\n",
      " 19  WORD_FREQ_YOU               4210 non-null   float64\n",
      " 20  WORD_FREQ_CREDIT            4210 non-null   float64\n",
      " 21  WORD_FREQ_YOUR              4210 non-null   float64\n",
      " 22  WORD_FREQ_FONT              4210 non-null   float64\n",
      " 23  WORD_FREQ_000               4210 non-null   float64\n",
      " 24  WORD_FREQ_MONEY             4210 non-null   float64\n",
      " 25  WORD_FREQ_HP                4210 non-null   float64\n",
      " 26  WORD_FREQ_HPL               4210 non-null   float64\n",
      " 27  WORD_FREQ_GEORGE            4210 non-null   float64\n",
      " 28  WORD_FREQ_650               4210 non-null   float64\n",
      " 29  WORD_FREQ_LAB               4210 non-null   float64\n",
      " 30  WORD_FREQ_LABS              4210 non-null   float64\n",
      " 31  WORD_FREQ_TELNET            4210 non-null   float64\n",
      " 32  WORD_FREQ_857               4210 non-null   float64\n",
      " 33  WORD_FREQ_DATA              4210 non-null   float64\n",
      " 34  WORD_FREQ_415               4210 non-null   float64\n",
      " 35  WORD_FREQ_85                4210 non-null   float64\n",
      " 36  WORD_FREQ_TECHNOLOGY        4210 non-null   float64\n",
      " 37  WORD_FREQ_1999              4210 non-null   float64\n",
      " 38  WORD_FREQ_PARTS             4210 non-null   float64\n",
      " 39  WORD_FREQ_PM                4210 non-null   float64\n",
      " 40  WORD_FREQ_DIRECT            4210 non-null   float64\n",
      " 41  WORD_FREQ_CS                4210 non-null   float64\n",
      " 42  WORD_FREQ_MEETING           4210 non-null   float64\n",
      " 43  WORD_FREQ_ORIGINAL          4210 non-null   float64\n",
      " 44  WORD_FREQ_PROJECT           4210 non-null   float64\n",
      " 45  WORD_FREQ_RE                4210 non-null   float64\n",
      " 46  WORD_FREQ_EDU               4210 non-null   float64\n",
      " 47  WORD_FREQ_TABLE             4210 non-null   float64\n",
      " 48  WORD_FREQ_CONFERENCE        4210 non-null   float64\n",
      " 49  CHAR_FREQ_;                 4210 non-null   float64\n",
      " 50  CHAR_FREQ_(                 4210 non-null   float64\n",
      " 51  CHAR_FREQ_[                 4210 non-null   float64\n",
      " 52  CHAR_FREQ_!                 4210 non-null   float64\n",
      " 53  CHAR_FREQ_$                 4210 non-null   float64\n",
      " 54  CHAR_FREQ_#                 4210 non-null   float64\n",
      " 55  CAPITAL_RUN_LENGTH_AVERAGE  4210 non-null   float64\n",
      " 56  CAPITAL_RUN_LENGTH_LONGEST  4210 non-null   float64\n",
      " 57  CAPITAL_RUN_LENGTH_TOTAL    4210 non-null   float64\n",
      " 58  SPAM                        4210 non-null   int64  \n",
      "dtypes: float64(57), int64(2)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "        Unnamed: 0  WORD_FREQ_MAKE  WORD_FREQ_ADDRESS  WORD_FREQ_ALL  \\\ncount  4210.000000    4.210000e+03       4.210000e+03   4.210000e+03   \nmean   2104.500000   -2.698337e-07       3.741093e-07  -2.019002e-08   \nstd    1215.466646    1.000119e+00       1.000119e+00   1.000119e+00   \nmin       0.000000   -3.479220e-01      -2.480270e-01  -5.652450e-01   \n25%    1052.250000   -3.479220e-01      -2.480270e-01  -5.652450e-01   \n50%    2104.500000   -3.479220e-01      -2.480270e-01  -5.652450e-01   \n75%    3156.750000   -3.479220e-01      -2.480270e-01   2.880350e-01   \nmax    4209.000000    1.478696e+01       3.119145e+01   9.325045e+00   \n\n       WORD_FREQ_3D  WORD_FREQ_OUR  WORD_FREQ_OVER  WORD_FREQ_REMOVE  \\\ncount  4.210000e+03   4.210000e+03    4.210000e+03      4.210000e+03   \nmean   3.536817e-07  -5.605701e-08   -3.327791e-07     -1.705463e-07   \nstd    1.000119e+00   1.000119e+00    1.000119e+00      1.000119e+00   \nmin   -4.664400e-02  -4.730400e-01   -3.502050e-01     -2.957310e-01   \n25%   -4.664400e-02  -4.730400e-01   -3.502050e-01     -2.957310e-01   \n50%   -4.664400e-02  -4.730400e-01   -3.502050e-01     -2.957310e-01   \n75%   -4.664400e-02   1.231300e-01   -3.502050e-01     -2.957310e-01   \nmax    3.160992e+01   1.406769e+01    2.095434e+01      1.800571e+01   \n\n       WORD_FREQ_INTERNET  WORD_FREQ_ORDER  ...   CHAR_FREQ_;   CHAR_FREQ_(  \\\ncount        4.210000e+03     4.210000e+03  ...  4.210000e+03  4.210000e+03   \nmean        -3.403800e-07    -1.337292e-07  ...  3.413302e-07  7.363420e-08   \nstd          1.000119e+00     1.000119e+00  ...  1.000119e+00  1.000119e+00   \nmin         -2.632650e-01    -3.256170e-01  ... -1.600100e-01 -5.252940e-01   \n25%         -2.632650e-01    -3.256170e-01  ... -1.600100e-01 -5.252940e-01   \n50%         -2.632650e-01    -3.256170e-01  ... -1.600100e-01 -2.590880e-01   \n75%         -2.632650e-01    -3.256170e-01  ... -1.600100e-01  1.821570e-01   \nmax          2.681886e+01     1.831957e+01  ...  1.720613e+01  3.503689e+01   \n\n        CHAR_FREQ_[   CHAR_FREQ_!   CHAR_FREQ_$   CHAR_FREQ_#  \\\ncount  4.210000e+03  4.210000e+03  4.210000e+03  4.210000e+03   \nmean   7.980998e-08 -1.218527e-07  2.353919e-07  6.840855e-08   \nstd    1.000119e+00  1.000119e+00  1.000119e+00  1.000119e+00   \nmin   -1.643640e-01 -3.334070e-01 -3.173250e-01 -1.050710e-01   \n25%   -1.643640e-01 -3.334070e-01 -3.173250e-01 -1.050710e-01   \n50%   -1.643640e-01 -3.144320e-01 -3.173250e-01 -1.050710e-01   \n75%   -1.643640e-01  5.913600e-02 -9.619700e-02 -1.050710e-01   \nmax    3.843832e+01  3.818318e+01  2.472857e+01  4.538756e+01   \n\n       CAPITAL_RUN_LENGTH_AVERAGE  CAPITAL_RUN_LENGTH_LONGEST  \\\ncount                4.210000e+03                4.210000e+03   \nmean                 2.612827e-08               -2.161520e-08   \nstd                  1.000119e+00                1.000119e+00   \nmin                 -1.322700e-01               -2.562650e-01   \n25%                 -1.133378e-01               -2.261990e-01   \n50%                 -9.313800e-02               -1.861100e-01   \n75%                 -5.060250e-02               -4.079000e-02   \nmax                  3.310208e+01                4.979423e+01   \n\n       CAPITAL_RUN_LENGTH_TOTAL         SPAM  \ncount              4.210000e+03  4210.000000  \nmean              -1.662708e-09     0.398812  \nstd                1.000119e+00     0.489712  \nmin               -4.691090e-01     0.000000  \n25%               -4.060610e-01     0.000000  \n50%               -3.066405e-01     0.000000  \n75%               -2.818025e-02     1.000000  \nmax                2.513787e+01     1.000000  \n\n[8 rows x 59 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>WORD_FREQ_MAKE</th>\n      <th>WORD_FREQ_ADDRESS</th>\n      <th>WORD_FREQ_ALL</th>\n      <th>WORD_FREQ_3D</th>\n      <th>WORD_FREQ_OUR</th>\n      <th>WORD_FREQ_OVER</th>\n      <th>WORD_FREQ_REMOVE</th>\n      <th>WORD_FREQ_INTERNET</th>\n      <th>WORD_FREQ_ORDER</th>\n      <th>...</th>\n      <th>CHAR_FREQ_;</th>\n      <th>CHAR_FREQ_(</th>\n      <th>CHAR_FREQ_[</th>\n      <th>CHAR_FREQ_!</th>\n      <th>CHAR_FREQ_$</th>\n      <th>CHAR_FREQ_#</th>\n      <th>CAPITAL_RUN_LENGTH_AVERAGE</th>\n      <th>CAPITAL_RUN_LENGTH_LONGEST</th>\n      <th>CAPITAL_RUN_LENGTH_TOTAL</th>\n      <th>SPAM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4210.000000</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>...</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4.210000e+03</td>\n      <td>4210.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2104.500000</td>\n      <td>-2.698337e-07</td>\n      <td>3.741093e-07</td>\n      <td>-2.019002e-08</td>\n      <td>3.536817e-07</td>\n      <td>-5.605701e-08</td>\n      <td>-3.327791e-07</td>\n      <td>-1.705463e-07</td>\n      <td>-3.403800e-07</td>\n      <td>-1.337292e-07</td>\n      <td>...</td>\n      <td>3.413302e-07</td>\n      <td>7.363420e-08</td>\n      <td>7.980998e-08</td>\n      <td>-1.218527e-07</td>\n      <td>2.353919e-07</td>\n      <td>6.840855e-08</td>\n      <td>2.612827e-08</td>\n      <td>-2.161520e-08</td>\n      <td>-1.662708e-09</td>\n      <td>0.398812</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1215.466646</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>...</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>1.000119e+00</td>\n      <td>0.489712</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-3.479220e-01</td>\n      <td>-2.480270e-01</td>\n      <td>-5.652450e-01</td>\n      <td>-4.664400e-02</td>\n      <td>-4.730400e-01</td>\n      <td>-3.502050e-01</td>\n      <td>-2.957310e-01</td>\n      <td>-2.632650e-01</td>\n      <td>-3.256170e-01</td>\n      <td>...</td>\n      <td>-1.600100e-01</td>\n      <td>-5.252940e-01</td>\n      <td>-1.643640e-01</td>\n      <td>-3.334070e-01</td>\n      <td>-3.173250e-01</td>\n      <td>-1.050710e-01</td>\n      <td>-1.322700e-01</td>\n      <td>-2.562650e-01</td>\n      <td>-4.691090e-01</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1052.250000</td>\n      <td>-3.479220e-01</td>\n      <td>-2.480270e-01</td>\n      <td>-5.652450e-01</td>\n      <td>-4.664400e-02</td>\n      <td>-4.730400e-01</td>\n      <td>-3.502050e-01</td>\n      <td>-2.957310e-01</td>\n      <td>-2.632650e-01</td>\n      <td>-3.256170e-01</td>\n      <td>...</td>\n      <td>-1.600100e-01</td>\n      <td>-5.252940e-01</td>\n      <td>-1.643640e-01</td>\n      <td>-3.334070e-01</td>\n      <td>-3.173250e-01</td>\n      <td>-1.050710e-01</td>\n      <td>-1.133378e-01</td>\n      <td>-2.261990e-01</td>\n      <td>-4.060610e-01</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2104.500000</td>\n      <td>-3.479220e-01</td>\n      <td>-2.480270e-01</td>\n      <td>-5.652450e-01</td>\n      <td>-4.664400e-02</td>\n      <td>-4.730400e-01</td>\n      <td>-3.502050e-01</td>\n      <td>-2.957310e-01</td>\n      <td>-2.632650e-01</td>\n      <td>-3.256170e-01</td>\n      <td>...</td>\n      <td>-1.600100e-01</td>\n      <td>-2.590880e-01</td>\n      <td>-1.643640e-01</td>\n      <td>-3.144320e-01</td>\n      <td>-3.173250e-01</td>\n      <td>-1.050710e-01</td>\n      <td>-9.313800e-02</td>\n      <td>-1.861100e-01</td>\n      <td>-3.066405e-01</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3156.750000</td>\n      <td>-3.479220e-01</td>\n      <td>-2.480270e-01</td>\n      <td>2.880350e-01</td>\n      <td>-4.664400e-02</td>\n      <td>1.231300e-01</td>\n      <td>-3.502050e-01</td>\n      <td>-2.957310e-01</td>\n      <td>-2.632650e-01</td>\n      <td>-3.256170e-01</td>\n      <td>...</td>\n      <td>-1.600100e-01</td>\n      <td>1.821570e-01</td>\n      <td>-1.643640e-01</td>\n      <td>5.913600e-02</td>\n      <td>-9.619700e-02</td>\n      <td>-1.050710e-01</td>\n      <td>-5.060250e-02</td>\n      <td>-4.079000e-02</td>\n      <td>-2.818025e-02</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4209.000000</td>\n      <td>1.478696e+01</td>\n      <td>3.119145e+01</td>\n      <td>9.325045e+00</td>\n      <td>3.160992e+01</td>\n      <td>1.406769e+01</td>\n      <td>2.095434e+01</td>\n      <td>1.800571e+01</td>\n      <td>2.681886e+01</td>\n      <td>1.831957e+01</td>\n      <td>...</td>\n      <td>1.720613e+01</td>\n      <td>3.503689e+01</td>\n      <td>3.843832e+01</td>\n      <td>3.818318e+01</td>\n      <td>2.472857e+01</td>\n      <td>4.538756e+01</td>\n      <td>3.310208e+01</td>\n      <td>4.979423e+01</td>\n      <td>2.513787e+01</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 59 columns</p>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0  WORD_FREQ_MAKE  WORD_FREQ_ADDRESS  WORD_FREQ_ALL  WORD_FREQ_3D  \\\n0           0       -0.347922           1.161025       0.675889     -0.046644   \n1           1        0.352150           0.368433       0.404391     -0.046644   \n2           2       -0.147901          -0.248027       0.811639     -0.046644   \n3           3       -0.347922          -0.248027      -0.565245     -0.046644   \n4           4       -0.347922          -0.248027      -0.565245     -0.046644   \n\n   WORD_FREQ_OUR  WORD_FREQ_OVER  WORD_FREQ_REMOVE  WORD_FREQ_INTERNET  \\\n0      -0.007737       -0.350205         -0.295731           -0.263265   \n1      -0.269470        0.664297          0.232922           -0.092630   \n2       1.315470        0.338207          0.182574            0.029252   \n3       0.443026       -0.350205          0.484661            1.272445   \n4       0.443026       -0.350205          0.484661            1.272445   \n\n   WORD_FREQ_ORDER  ...  CHAR_FREQ_;  CHAR_FREQ_(  CHAR_FREQ_[  CHAR_FREQ_!  \\\n0        -0.325617  ...    -0.160010    -0.525294    -0.164364     0.589246   \n1        -0.325617  ...    -0.160010    -0.043936    -0.164364     0.107759   \n2         1.943000  ...    -0.120407    -0.003822    -0.164364    -0.006090   \n3         0.773244  ...    -0.160010    -0.025702    -0.164364    -0.170935   \n4         0.773244  ...    -0.160010    -0.032996    -0.164364    -0.173306   \n\n   CHAR_FREQ_$  CHAR_FREQ_#  CAPITAL_RUN_LENGTH_AVERAGE  \\\n0    -0.317325    -0.105071                   -0.049117   \n1     0.433676     0.005053                   -0.008143   \n2     0.450365    -0.082129                    0.133876   \n3    -0.317325    -0.105071                   -0.055724   \n4    -0.317325    -0.105071                   -0.055724   \n\n   CAPITAL_RUN_LENGTH_LONGEST  CAPITAL_RUN_LENGTH_TOTAL  SPAM  \n0                    0.044398                 -0.021310     1  \n1                    0.244841                  1.191142     1  \n2                    2.169089                  3.181179     1  \n3                   -0.060834                 -0.161954     1  \n4                   -0.060834                 -0.161954     1  \n\n[5 rows x 59 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>WORD_FREQ_MAKE</th>\n      <th>WORD_FREQ_ADDRESS</th>\n      <th>WORD_FREQ_ALL</th>\n      <th>WORD_FREQ_3D</th>\n      <th>WORD_FREQ_OUR</th>\n      <th>WORD_FREQ_OVER</th>\n      <th>WORD_FREQ_REMOVE</th>\n      <th>WORD_FREQ_INTERNET</th>\n      <th>WORD_FREQ_ORDER</th>\n      <th>...</th>\n      <th>CHAR_FREQ_;</th>\n      <th>CHAR_FREQ_(</th>\n      <th>CHAR_FREQ_[</th>\n      <th>CHAR_FREQ_!</th>\n      <th>CHAR_FREQ_$</th>\n      <th>CHAR_FREQ_#</th>\n      <th>CAPITAL_RUN_LENGTH_AVERAGE</th>\n      <th>CAPITAL_RUN_LENGTH_LONGEST</th>\n      <th>CAPITAL_RUN_LENGTH_TOTAL</th>\n      <th>SPAM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-0.347922</td>\n      <td>1.161025</td>\n      <td>0.675889</td>\n      <td>-0.046644</td>\n      <td>-0.007737</td>\n      <td>-0.350205</td>\n      <td>-0.295731</td>\n      <td>-0.263265</td>\n      <td>-0.325617</td>\n      <td>...</td>\n      <td>-0.160010</td>\n      <td>-0.525294</td>\n      <td>-0.164364</td>\n      <td>0.589246</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.049117</td>\n      <td>0.044398</td>\n      <td>-0.021310</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.352150</td>\n      <td>0.368433</td>\n      <td>0.404391</td>\n      <td>-0.046644</td>\n      <td>-0.269470</td>\n      <td>0.664297</td>\n      <td>0.232922</td>\n      <td>-0.092630</td>\n      <td>-0.325617</td>\n      <td>...</td>\n      <td>-0.160010</td>\n      <td>-0.043936</td>\n      <td>-0.164364</td>\n      <td>0.107759</td>\n      <td>0.433676</td>\n      <td>0.005053</td>\n      <td>-0.008143</td>\n      <td>0.244841</td>\n      <td>1.191142</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-0.147901</td>\n      <td>-0.248027</td>\n      <td>0.811639</td>\n      <td>-0.046644</td>\n      <td>1.315470</td>\n      <td>0.338207</td>\n      <td>0.182574</td>\n      <td>0.029252</td>\n      <td>1.943000</td>\n      <td>...</td>\n      <td>-0.120407</td>\n      <td>-0.003822</td>\n      <td>-0.164364</td>\n      <td>-0.006090</td>\n      <td>0.450365</td>\n      <td>-0.082129</td>\n      <td>0.133876</td>\n      <td>2.169089</td>\n      <td>3.181179</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>-0.347922</td>\n      <td>-0.248027</td>\n      <td>-0.565245</td>\n      <td>-0.046644</td>\n      <td>0.443026</td>\n      <td>-0.350205</td>\n      <td>0.484661</td>\n      <td>1.272445</td>\n      <td>0.773244</td>\n      <td>...</td>\n      <td>-0.160010</td>\n      <td>-0.025702</td>\n      <td>-0.164364</td>\n      <td>-0.170935</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.055724</td>\n      <td>-0.060834</td>\n      <td>-0.161954</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>-0.347922</td>\n      <td>-0.248027</td>\n      <td>-0.565245</td>\n      <td>-0.046644</td>\n      <td>0.443026</td>\n      <td>-0.350205</td>\n      <td>0.484661</td>\n      <td>1.272445</td>\n      <td>0.773244</td>\n      <td>...</td>\n      <td>-0.160010</td>\n      <td>-0.032996</td>\n      <td>-0.164364</td>\n      <td>-0.173306</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.055724</td>\n      <td>-0.060834</td>\n      <td>-0.161954</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 59 columns</p>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0  WORD_FREQ_MAKE  WORD_FREQ_ADDRESS  WORD_FREQ_ALL  WORD_FREQ_3D  \\\n0           0       -0.347922           1.161025       0.675889     -0.046644   \n1           1        0.352150           0.368433       0.404391     -0.046644   \n2           2       -0.147901          -0.248027       0.811639     -0.046644   \n3           3       -0.347922          -0.248027      -0.565245     -0.046644   \n4           4       -0.347922          -0.248027      -0.565245     -0.046644   \n\n   WORD_FREQ_OUR  WORD_FREQ_OVER  WORD_FREQ_REMOVE  WORD_FREQ_INTERNET  \\\n0      -0.007737       -0.350205         -0.295731           -0.263265   \n1      -0.269470        0.664297          0.232922           -0.092630   \n2       1.315470        0.338207          0.182574            0.029252   \n3       0.443026       -0.350205          0.484661            1.272445   \n4       0.443026       -0.350205          0.484661            1.272445   \n\n   WORD_FREQ_ORDER  ...  WORD_FREQ_TABLE  WORD_FREQ_CONFERENCE  CHAR_FREQ_;  \\\n0        -0.325617  ...        -0.073094             -0.116407    -0.160010   \n1        -0.325617  ...        -0.073094             -0.116407    -0.160010   \n2         1.943000  ...        -0.073094             -0.116407    -0.120407   \n3         0.773244  ...        -0.073094             -0.116407    -0.160010   \n4         0.773244  ...        -0.073094             -0.116407    -0.160010   \n\n   CHAR_FREQ_(  CHAR_FREQ_[  CHAR_FREQ_!  CHAR_FREQ_$  CHAR_FREQ_#  \\\n0    -0.525294    -0.164364     0.589246    -0.317325    -0.105071   \n1    -0.043936    -0.164364     0.107759     0.433676     0.005053   \n2    -0.003822    -0.164364    -0.006090     0.450365    -0.082129   \n3    -0.025702    -0.164364    -0.170935    -0.317325    -0.105071   \n4    -0.032996    -0.164364    -0.173306    -0.317325    -0.105071   \n\n   CAPITAL_RUN_LENGTH_AVERAGE  CAPITAL_RUN_LENGTH_LONGEST  \n0                   -0.049117                    0.044398  \n1                   -0.008143                    0.244841  \n2                    0.133876                    2.169089  \n3                   -0.055724                   -0.060834  \n4                   -0.055724                   -0.060834  \n\n[5 rows x 57 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>WORD_FREQ_MAKE</th>\n      <th>WORD_FREQ_ADDRESS</th>\n      <th>WORD_FREQ_ALL</th>\n      <th>WORD_FREQ_3D</th>\n      <th>WORD_FREQ_OUR</th>\n      <th>WORD_FREQ_OVER</th>\n      <th>WORD_FREQ_REMOVE</th>\n      <th>WORD_FREQ_INTERNET</th>\n      <th>WORD_FREQ_ORDER</th>\n      <th>...</th>\n      <th>WORD_FREQ_TABLE</th>\n      <th>WORD_FREQ_CONFERENCE</th>\n      <th>CHAR_FREQ_;</th>\n      <th>CHAR_FREQ_(</th>\n      <th>CHAR_FREQ_[</th>\n      <th>CHAR_FREQ_!</th>\n      <th>CHAR_FREQ_$</th>\n      <th>CHAR_FREQ_#</th>\n      <th>CAPITAL_RUN_LENGTH_AVERAGE</th>\n      <th>CAPITAL_RUN_LENGTH_LONGEST</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-0.347922</td>\n      <td>1.161025</td>\n      <td>0.675889</td>\n      <td>-0.046644</td>\n      <td>-0.007737</td>\n      <td>-0.350205</td>\n      <td>-0.295731</td>\n      <td>-0.263265</td>\n      <td>-0.325617</td>\n      <td>...</td>\n      <td>-0.073094</td>\n      <td>-0.116407</td>\n      <td>-0.160010</td>\n      <td>-0.525294</td>\n      <td>-0.164364</td>\n      <td>0.589246</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.049117</td>\n      <td>0.044398</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.352150</td>\n      <td>0.368433</td>\n      <td>0.404391</td>\n      <td>-0.046644</td>\n      <td>-0.269470</td>\n      <td>0.664297</td>\n      <td>0.232922</td>\n      <td>-0.092630</td>\n      <td>-0.325617</td>\n      <td>...</td>\n      <td>-0.073094</td>\n      <td>-0.116407</td>\n      <td>-0.160010</td>\n      <td>-0.043936</td>\n      <td>-0.164364</td>\n      <td>0.107759</td>\n      <td>0.433676</td>\n      <td>0.005053</td>\n      <td>-0.008143</td>\n      <td>0.244841</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-0.147901</td>\n      <td>-0.248027</td>\n      <td>0.811639</td>\n      <td>-0.046644</td>\n      <td>1.315470</td>\n      <td>0.338207</td>\n      <td>0.182574</td>\n      <td>0.029252</td>\n      <td>1.943000</td>\n      <td>...</td>\n      <td>-0.073094</td>\n      <td>-0.116407</td>\n      <td>-0.120407</td>\n      <td>-0.003822</td>\n      <td>-0.164364</td>\n      <td>-0.006090</td>\n      <td>0.450365</td>\n      <td>-0.082129</td>\n      <td>0.133876</td>\n      <td>2.169089</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>-0.347922</td>\n      <td>-0.248027</td>\n      <td>-0.565245</td>\n      <td>-0.046644</td>\n      <td>0.443026</td>\n      <td>-0.350205</td>\n      <td>0.484661</td>\n      <td>1.272445</td>\n      <td>0.773244</td>\n      <td>...</td>\n      <td>-0.073094</td>\n      <td>-0.116407</td>\n      <td>-0.160010</td>\n      <td>-0.025702</td>\n      <td>-0.164364</td>\n      <td>-0.170935</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.055724</td>\n      <td>-0.060834</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>-0.347922</td>\n      <td>-0.248027</td>\n      <td>-0.565245</td>\n      <td>-0.046644</td>\n      <td>0.443026</td>\n      <td>-0.350205</td>\n      <td>0.484661</td>\n      <td>1.272445</td>\n      <td>0.773244</td>\n      <td>...</td>\n      <td>-0.073094</td>\n      <td>-0.116407</td>\n      <td>-0.160010</td>\n      <td>-0.032996</td>\n      <td>-0.164364</td>\n      <td>-0.173306</td>\n      <td>-0.317325</td>\n      <td>-0.105071</td>\n      <td>-0.055724</td>\n      <td>-0.060834</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 57 columns</p>\n</div>"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data.iloc[:,0:57]\n",
    "x.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "0    1\n1    1\n2    1\n3    1\n4    1\nName: SPAM, dtype: int64"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.iloc[:,-1]\n",
    "y.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.73163944e+00, -3.47921694e-01,  1.16102473e+00,\n         6.75889009e-01, -4.66443548e-02, -7.73694379e-03,\n        -3.50204627e-01, -2.95730813e-01, -2.63264636e-01,\n        -3.25616851e-01, -3.78366309e-01, -3.07836420e-01,\n         8.47631264e-02, -3.15758927e-01, -1.76898987e-01,\n        -1.85015569e-01,  8.29790727e-02, -3.25938850e-01,\n         2.03531292e+00,  1.19664108e-01, -1.68067636e-01,\n         1.30489119e-01, -1.22848356e-01, -2.86432866e-01,\n        -2.10737045e-01, -3.42409901e-01, -3.08500363e-01,\n        -2.08345727e-01, -2.40162603e-01, -1.70587229e-01,\n        -2.36121176e-01, -1.64102339e-01, -1.49368148e-01,\n        -1.79821647e-01, -1.51690935e-01, -2.03472801e-01,\n        -2.57147815e-01, -3.38036553e-01, -6.15660341e-02,\n        -1.87653192e-01, -1.91804015e-01, -1.30233188e-01,\n        -1.78329236e-01, -2.10365694e-01, -1.31986844e-01,\n        -3.06106866e-01, -2.04024811e-01, -7.30943168e-02,\n        -1.16407083e-01, -1.60010349e-01, -5.25294094e-01,\n        -1.64364081e-01,  5.89246099e-01, -3.17325260e-01,\n        -1.05071069e-01, -4.91170260e-02,  4.43980215e-02],\n       [-1.73081662e+00,  3.52150233e-01,  3.68432660e-01,\n         4.04391014e-01, -4.66443548e-02, -2.69469939e-01,\n         6.64297257e-01,  2.32922157e-01, -9.26296513e-02,\n        -3.25616851e-01,  1.05333683e+00,  8.32570698e-01,\n         2.54753108e-01,  1.78594704e+00,  4.29994007e-01,\n         3.93121385e-01, -1.42742922e-01, -1.72971857e-01,\n         1.68637791e-01,  9.90433989e-01, -1.68067636e-01,\n         6.78689070e-01, -1.22848356e-01,  9.37611097e-01,\n         7.85390959e-01, -3.42409901e-01, -3.08500363e-01,\n        -2.08345727e-01, -2.40162603e-01, -1.70587229e-01,\n        -2.36121176e-01, -1.64102339e-01, -1.49368148e-01,\n        -1.79821647e-01, -1.51690935e-01, -2.03472801e-01,\n        -2.57147815e-01, -1.73119575e-01, -6.15660341e-02,\n        -1.87653192e-01, -1.91804015e-01, -1.30233188e-01,\n        -1.78329236e-01, -2.10365694e-01, -1.31986844e-01,\n        -3.06106866e-01, -2.04024811e-01, -7.30943168e-02,\n        -1.16407083e-01, -1.60010349e-01, -4.39360753e-02,\n        -1.64364081e-01,  1.07759118e-01,  4.33675799e-01,\n         5.05293161e-03, -8.14302611e-03,  2.44841021e-01],\n       [-1.72999379e+00, -1.47900715e-01, -2.48027397e-01,\n         8.11639007e-01, -4.66443548e-02,  1.31547003e+00,\n         3.38207294e-01,  1.82574160e-01,  2.92523377e-02,\n         1.94300004e+00,  2.40572850e-03,  1.75575779e+00,\n        -1.30556851e-01,  7.22480674e-02, -1.76898987e-01,\n         7.04169486e+00, -2.43063920e-01, -1.94823856e-01,\n         1.55478189e+00, -2.02633848e-01,  4.64941326e-01,\n        -2.61082845e-01, -1.22848356e-01,  3.01563904e+00,\n        -7.17420440e-02, -3.42409901e-01, -3.08500363e-01,\n        -2.08345727e-01, -2.40162603e-01, -1.70587229e-01,\n        -2.36121176e-01, -1.64102339e-01, -1.49368148e-01,\n        -1.79821647e-01, -1.51690935e-01, -2.03472801e-01,\n        -2.57147815e-01, -3.38036553e-01, -6.15660341e-02,\n        -1.87653192e-01, -4.00001404e-03, -1.30233188e-01,\n        -1.78329236e-01,  3.07224277e-01, -1.31986844e-01,\n        -2.48733868e-01, -1.39382814e-01, -7.30943168e-02,\n        -1.16407083e-01, -1.20407347e-01, -3.82207378e-03,\n        -1.64364081e-01, -6.08987791e-03,  4.50364800e-01,\n        -8.21290688e-02,  1.33875974e-01,  2.16908902e+00],\n       [-1.72917096e+00, -3.47921694e-01, -2.48027397e-01,\n        -5.65244971e-01, -4.66443548e-02,  4.43026047e-01,\n        -3.50204627e-01,  4.84661143e-01,  1.27244523e+00,\n         7.73244097e-01,  5.81179786e-01,  1.37562175e+00,\n        -2.89212834e-01,  6.86593059e-01, -1.76898987e-01,\n        -1.85015569e-01,  7.04390730e-02, -3.25938850e-01,\n        -3.48856244e-01,  8.26458011e-01, -1.68067636e-01,\n        -4.35114830e-01, -1.22848356e-01, -2.86432866e-01,\n        -2.10737045e-01, -3.42409901e-01, -3.08500363e-01,\n        -2.08345727e-01, -2.40162603e-01, -1.70587229e-01,\n        -2.36121176e-01, -1.64102339e-01, -1.49368148e-01,\n        -1.79821647e-01, -1.51690935e-01, -2.03472801e-01,\n        -2.57147815e-01, -3.38036553e-01, -6.15660341e-02,\n        -1.87653192e-01, -1.91804015e-01, -1.30233188e-01,\n        -1.78329236e-01, -2.10365694e-01, -1.31986844e-01,\n        -3.06106866e-01, -2.04024811e-01, -7.30943168e-02,\n        -1.16407083e-01, -1.60010349e-01, -2.57020746e-02,\n        -1.64364081e-01, -1.70934871e-01, -3.17325260e-01,\n        -1.05071069e-01, -5.57240260e-02, -6.08339783e-02],\n       [-1.72834813e+00, -3.47921694e-01, -2.48027397e-01,\n        -5.65244971e-01, -4.66443548e-02,  4.43026047e-01,\n        -3.50204627e-01,  4.84661143e-01,  1.27244523e+00,\n         7.73244097e-01,  5.81179786e-01,  1.37562175e+00,\n        -2.89212834e-01,  6.86593059e-01, -1.76898987e-01,\n        -1.85015569e-01,  7.04390730e-02, -3.25938850e-01,\n        -3.48856244e-01,  8.26458011e-01, -1.68067636e-01,\n        -4.35114830e-01, -1.22848356e-01, -2.86432866e-01,\n        -2.10737045e-01, -3.42409901e-01, -3.08500363e-01,\n        -2.08345727e-01, -2.40162603e-01, -1.70587229e-01,\n        -2.36121176e-01, -1.64102339e-01, -1.49368148e-01,\n        -1.79821647e-01, -1.51690935e-01, -2.03472801e-01,\n        -2.57147815e-01, -3.38036553e-01, -6.15660341e-02,\n        -1.87653192e-01, -1.91804015e-01, -1.30233188e-01,\n        -1.78329236e-01, -2.10365694e-01, -1.31986844e-01,\n        -3.06106866e-01, -2.04024811e-01, -7.30943168e-02,\n        -1.16407083e-01, -1.60010349e-01, -3.29960749e-02,\n        -1.64364081e-01, -1.73305871e-01, -3.17325260e-01,\n        -1.05071069e-01, -5.57240260e-02, -6.08339783e-02]])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = scaler.fit_transform(x)\n",
    "x[0:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "(3368, 57)"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "(842, 57)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "KNeighborsClassifier(n_neighbors=1)",
      "text/html": "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=1)</pre></div></div></div></div></div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 1, 1, 0], dtype=int64)"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "pred[0:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "3134    0\n2500    0\n776     1\n11      1\n3505    0\nName: SPAM, dtype: int64"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9584323040380047"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test,pred)\n",
    "accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[485,  20],\n       [ 15, 322]], dtype=int64)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,pred)\n",
    "cm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "     y_test  pred\n0         0     0\n1         0     0\n2         1     1\n3         1     1\n4         0     0\n..      ...   ...\n837       0     0\n838       1     1\n839       0     0\n840       1     1\n841       0     0\n\n[842 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y_test</th>\n      <th>pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>837</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>838</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>839</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>840</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>841</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>842 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(data=[y_test.values],index=[\"y_test\",\"pred\"])\n",
    "results.transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "correct_sum = []\n",
    "for i in range(1,30):\n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "    model.fit(x_train,y_train)\n",
    "    pred = model.predict(x_test)\n",
    "    correct = np.sum(pred==y_test)\n",
    "    correct_sum.append(correct)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "[807,\n 791,\n 808,\n 802,\n 807,\n 807,\n 805,\n 803,\n 806,\n 807,\n 813,\n 809,\n 805,\n 804,\n 805,\n 803,\n 802,\n 801,\n 800,\n 803,\n 803,\n 805,\n 802,\n 803,\n 804,\n 801,\n 801,\n 802,\n 802]"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "    1    2    3    4    5    6    7    8    9    10  ...   20   21   22   23  \\\n0  807  791  808  802  807  807  805  803  806  807  ...  803  803  805  802   \n\n    24   25   26   27   28   29  \n0  803  804  801  801  802  802  \n\n[1 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>807</td>\n      <td>791</td>\n      <td>808</td>\n      <td>802</td>\n      <td>807</td>\n      <td>807</td>\n      <td>805</td>\n      <td>803</td>\n      <td>806</td>\n      <td>807</td>\n      <td>...</td>\n      <td>803</td>\n      <td>803</td>\n      <td>805</td>\n      <td>802</td>\n      <td>803</td>\n      <td>804</td>\n      <td>801</td>\n      <td>801</td>\n      <td>802</td>\n      <td>802</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 29 columns</p>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(data=correct_sum)\n",
    "results.index = results.index+1\n",
    "results.T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9584323040380047"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
